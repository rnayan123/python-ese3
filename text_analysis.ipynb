{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['Absolutely', 'wonderful', '-', 'silky', 'and', 'sexy', 'and', 'comfortable', 'Love', 'this', 'dress', '!', 'it', \"'s\", 'sooo', 'pretty', '.', 'i', 'happened', 'to', 'find', 'it', 'in', 'a', 'store', ',', 'and', 'i', \"'m\", 'glad', 'i', 'did', 'bc', 'i', 'never', 'would', 'have', 'ordered', 'it', 'online', 'bc', 'it', \"'s\", 'petite', '.', 'i', 'bought', 'a', 'petite', 'and', 'am', '5', \"'\", '8', \"''\", '.', 'i', 'love', 'the', 'length', 'on', 'me-', 'hits', 'just', 'a', 'little', 'below', 'the', 'knee', '.', 'would', 'definitely', 'be', 'a', 'true', 'midi', 'on', 'someone', 'who', 'is', 'truly', 'petite', '.', 'I', 'had', 'such', 'high', 'hopes', 'for', 'this', 'dress', 'and', 'really', 'wanted', 'it', 'to', 'work', 'for', 'me', '.']\n",
      "1573232\n",
      "\n",
      "Removing stopwords:\n",
      "['Absolutely', 'wonderful', '-', 'silky', 'sexy', 'comfortable', 'Love', 'dress', '!', \"'s\", 'sooo', 'pretty', '.', 'happened', 'find', 'store', ',', \"'m\", 'glad', 'bc', 'never', 'would', 'ordered', 'online', 'bc', \"'s\", 'petite', '.', 'bought', 'petite', '5', \"'\", '8', \"''\", '.', 'love', 'length', 'me-', 'hits', 'little', 'knee', '.', 'would', 'definitely', 'true', 'midi', 'someone', 'truly', 'petite', '.', 'high', 'hopes', 'dress', 'really', 'wanted', 'work', '.', 'initially', 'ordered', 'petite', 'small', '(', 'usual', 'size', ')', 'found', 'outrageously', 'small', '.', 'small', 'fact', 'could', 'zip', '!', 'reordered', 'petite', 'medium', ',', 'ok.', 'overall', ',', 'top', 'half', 'comfortable', 'fit', 'nicely', ',', 'bottom', 'half', 'tight', 'layer', 'several', 'somewhat', 'cheap', '(', 'net', ')', 'layers', '.', 'imo', ',', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper', '-', 'c', 'love', ',', 'love', ',', 'love', 'jumpsuit', '.', \"'s\", 'fun', ',', 'flirty', ',', 'fabulous', '!', 'every', 'time', 'wear', ',', 'get', 'nothing', 'great', 'compliments', '!', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', '.', 'perfect', 'length', 'wear', 'leggings', 'sleeveless', 'pairs', 'well', 'cardigan', '.', 'love', 'shirt', '!', '!', '!', 'love', 'tracy', 'reese', 'dresses', ',', 'one', 'petite', '.', '5', 'feet', 'tall', 'usually', 'wear', '0p', 'brand', '.', 'dress', 'pretty', 'package', 'lot', 'dress', '.', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', '.', 'stranger', 'alterations', ',', 'shortening', 'narrowing', 'skirt', 'would', 'take', 'away', 'embellishment', 'garment', '.', 'love', 'color', 'idea', 'style']\n",
      "866116\n",
      "\n",
      "Stemming:\n",
      "['absolut', 'wonder', '-', 'silki', 'sexi', 'comfort', 'love', 'dress', '!', \"'s\", 'sooo', 'pretti', '.', 'happen', 'find', 'store', ',', \"'m\", 'glad', 'bc', 'never', 'would', 'order', 'onlin', 'bc', \"'s\", 'petit', '.', 'bought', 'petit', '5', \"'\", '8', \"''\", '.', 'love', 'length', 'me-', 'hit', 'littl', 'knee', '.', 'would', 'definit', 'true', 'midi', 'someon', 'truli', 'petit', '.', 'high', 'hope', 'dress', 'realli', 'want', 'work', '.', 'initi', 'order', 'petit', 'small', '(', 'usual', 'size', ')', 'found', 'outrag', 'small', '.', 'small', 'fact', 'could', 'zip', '!', 'reorder', 'petit', 'medium', ',', 'ok.', 'overal', ',', 'top', 'half', 'comfort', 'fit', 'nice', ',', 'bottom', 'half', 'tight', 'layer', 'sever', 'somewhat', 'cheap', '(', 'net', ')', 'layer', '.', 'imo']\n",
      "866116\n",
      "\n",
      "Lemmatization:\n",
      "['Absolutely', 'wonderful', '-', 'silky', 'sexy', 'comfortable', 'Love', 'dress', '!', \"'s\", 'sooo', 'pretty', '.', 'happened', 'find', 'store', ',', \"'m\", 'glad', 'bc', 'never', 'would', 'ordered', 'online', 'bc', \"'s\", 'petite', '.', 'bought', 'petite', '5', \"'\", '8', \"''\", '.', 'love', 'length', 'me-', 'hit', 'little', 'knee', '.', 'would', 'definitely', 'true', 'midi', 'someone', 'truly', 'petite', '.', 'high', 'hope', 'dress', 'really', 'wanted', 'work', '.', 'initially', 'ordered', 'petite', 'small', '(', 'usual', 'size', ')', 'found', 'outrageously', 'small', '.', 'small', 'fact', 'could', 'zip', '!', 'reordered', 'petite', 'medium', ',', 'ok.', 'overall', ',', 'top', 'half', 'comfortable', 'fit', 'nicely', ',', 'bottom', 'half', 'tight', 'layer', 'several', 'somewhat', 'cheap', '(', 'net', ')', 'layer', '.', 'imo', ',', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper', '-', 'c', 'love', ',', 'love', ',', 'love', 'jumpsuit', '.', \"'s\", 'fun', ',', 'flirty', ',', 'fabulous', '!', 'every', 'time', 'wear', ',', 'get', 'nothing', 'great', 'compliment', '!', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', '.', 'perfect', 'length', 'wear', 'legging', 'sleeveless', 'pair', 'well', 'cardigan', '.', 'love', 'shirt', '!', '!', '!', 'love', 'tracy', 'reese', 'dress', ',', 'one', 'petite', '.', '5', 'foot', 'tall', 'usually', 'wear', '0p', 'brand', '.', 'dress', 'pretty', 'package', 'lot', 'dress', '.', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', '.', 'stranger', 'alteration', ',', 'shortening', 'narrowing', 'skirt', 'would', 'take', 'away', 'embellishment', 'garment', '.', 'love', 'color', 'idea', 'style']\n",
      "866116\n",
      "\n",
      "Cleaned text:\n",
      "['Absolutely', 'wonderful', 'silky', 'sexy', 'comfortable', 'Love', 'dress', 'sooo', 'pretty', 'happened', 'find', 'store', 'glad', 'bc', 'never', 'would', 'ordered', 'online', 'bc', 'petite', 'bought', 'petite', 'love', 'length', 'hit', 'little', 'knee', 'would', 'definitely', 'true', 'midi', 'someone', 'truly', 'petite', 'high', 'hope', 'dress', 'really', 'wanted', 'work', 'initially', 'ordered', 'petite', 'small', 'usual', 'size', 'found', 'outrageously', 'small', 'small', 'fact', 'could', 'zip', 'reordered', 'petite', 'medium', 'overall', 'top', 'half', 'comfortable', 'fit', 'nicely', 'bottom', 'half', 'tight', 'layer', 'several', 'somewhat', 'cheap', 'net', 'layer', 'imo', 'major', 'design', 'flaw', 'net', 'layer', 'sewn', 'directly', 'zipper', 'c', 'love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'every', 'time', 'wear', 'get', 'nothing', 'great', 'compliment', 'shirt', 'flattering', 'due', 'adjustable', 'front', 'tie', 'perfect', 'length', 'wear', 'legging', 'sleeveless', 'pair', 'well', 'cardigan', 'love', 'shirt', 'love', 'tracy', 'reese', 'dress', 'one', 'petite', 'foot', 'tall', 'usually', 'wear', 'brand', 'dress', 'pretty', 'package', 'lot', 'dress', 'skirt', 'long', 'full', 'overwhelmed', 'small', 'frame', 'stranger', 'alteration', 'shortening', 'narrowing', 'skirt', 'would', 'take', 'away', 'embellishment', 'garment', 'love', 'color', 'idea', 'style', 'work', 'returned', 'dress', 'aded', 'basket', 'hte', 'last', 'mintue', 'see', 'would', 'look', 'like', 'person', 'store', 'pick', 'went', 'teh', 'darkler', 'color', 'pale', 'hte', 'color', 'really', 'gorgeous', 'turn', 'mathced', 'everythiing', 'trying', 'prefectly', 'little', 'baggy', 'hte', 'x', 'hte', 'msallet', 'size', 'bummer', 'petite', 'decided', 'jkeep', 'though', 'said', 'matvehd', 'everything', 'ejans', 'pant', 'skirt', 'waas', 'trying', 'kept', 'oops', 'ordered', 'carbon']\n",
      "625284\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df = pd.read_csv(\"WomensClothingE-CommerceReviews.csv\")\n",
    "df.head()\n",
    "\n",
    "# Tokenization\n",
    "print(\"Tokenization:\")\n",
    "tokens = nltk.word_tokenize(df['Review Text'].str.cat(sep=' '))\n",
    "print(tokens[:100]) \n",
    "print(len(tokens))\n",
    "\n",
    "#Remove stopwords\n",
    "print(\"\\nRemoving stopwords:\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens[:200])\n",
    "print(len(filtered_tokens))\n",
    "\n",
    "# Stemming\n",
    "print(\"\\nStemming:\")\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed_tokens[:100])\n",
    "print(len(stemmed_tokens))\n",
    "\n",
    "# Lemmatization\n",
    "print(\"\\nLemmatization:\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmatized_tokens[:200])\n",
    "print(len(lemmatized_tokens))\n",
    "\n",
    "# Clean text (remove special characters, punctuation, and numbers)\n",
    "print(\"\\nCleaned text:\")\n",
    "cleaned_tokens = [word for word in lemmatized_tokens if word.isalpha()]\n",
    "print(cleaned_tokens[:200])\n",
    "print(len(cleaned_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1,set2):\n",
    "    intersection=len(set1.intersection(set2))\n",
    "    union=len(set1.union(set2))\n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'intersection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29692\\2427975668.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimilarity_score1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjaccard_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Division Name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"general\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Division Name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"General Petite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Jaccard similarity: {similarity_score1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29692\\1937854717.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(set1, set2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjaccard_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mintersection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0munion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mintersection\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'intersection'"
     ]
    }
   ],
   "source": [
    "similarity_score1 = jaccard_similarity(df[])\n",
    "print(f\"Jaccard similarity: {similarity_score1}\"\n",
    "#using jaccard similarity implement a text similarity analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
